# -*- coding: utf-8 -*-
"""Lung Cancer Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NYK0DdocHqjajHVBxrRKpzfNJxhMr8PK
"""

# ===============================
# 1. Setup Kaggle API
# ===============================
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# ===============================
# 2. Download & Extract Dataset
#   Example: Lung & Colon Cancer Histopathology Images
# ===============================
!kaggle datasets download -d andrewmvd/lung-and-colon-cancer-histopathological-images -p /content
import zipfile
zip_ref = zipfile.ZipFile('/content/lung-and-colon-cancer-histopathological-images.zip','r')
zip_ref.extractall('/content/data')
zip_ref.close()

!ls /content/data/lung_colon_image_set/lung_image_sets

# ===============================
# 3. Prepare Train/Val/Test Split
# ===============================
import os, glob, random, shutil

SRC = "/content/data/lung_colon_image_set/lung_image_sets"
TARGET = "/content/dataset"
splits = ["train","val","test"]
classes = ["lung_aca","lung_scc","lung_n"]

for sp in splits:
    for cls in classes:
        os.makedirs(os.path.join(TARGET, sp, cls), exist_ok=True)

# 70/15/15 split
random.seed(42)
for cls in classes:
    files = glob.glob(os.path.join(SRC, cls, "*.jpeg"))
    random.shuffle(files)
    n = len(files)
    n_train, n_val = int(0.7*n), int(0.15*n)
    train, val, test = files[:n_train], files[n_train:n_train+n_val], files[n_train+n_val:]
    for f in train: shutil.copy(f, os.path.join(TARGET,"train",cls))
    for f in val:   shutil.copy(f, os.path.join(TARGET,"val",cls))
    for f in test:  shutil.copy(f, os.path.join(TARGET,"test",cls))

# ===============================
# 4. Load Datasets
# ===============================
import tensorflow as tf
from tensorflow.keras import layers, models

IMG_SIZE = (224,224)
BATCH_SIZE = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(TARGET,"train"), image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode="categorical")
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(TARGET,"val"), image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode="categorical")
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    os.path.join(TARGET,"test"), image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode="categorical", shuffle=False)

class_names = train_ds.class_names
print("Classes:", class_names)

# ===============================
# 5. Build Model (EfficientNetB0)
# ===============================
from tensorflow.keras.applications import EfficientNetB0

base = EfficientNetB0(include_top=False, weights="imagenet", input_shape=IMG_SIZE+(3,))
base.trainable = False   # freeze for first stage

inputs = layers.Input(shape=IMG_SIZE+(3,))
x = tf.keras.applications.efficientnet.preprocess_input(inputs)
x = base(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(len(class_names), activation="softmax")(x)
model = models.Model(inputs, outputs)

model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()

# ===============================
# 6. Train (frozen base)
# ===============================
history = model.fit(train_ds, validation_data=val_ds, epochs=5)

# ===============================
# 7. Fine-Tuning (unfreeze top layers)
# ===============================
base.trainable = True
for layer in base.layers[:-40]:  # unfreeze last 40 layers only
    layer.trainable = False

model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss="categorical_crossentropy", metrics=["accuracy"])

history_ft = model.fit(train_ds, validation_data=val_ds, epochs=5)

# ===============================
# 8. Evaluate
# ===============================
loss, acc = model.evaluate(test_ds)
print("âœ… Test Accuracy:", acc)

# ===============================
# 9. Confusion Matrix & Report
# ===============================
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import itertools
import numpy as np

# Ground truth and predictions
y_true = []
for _, labels in test_ds:
    y_true.append(labels.numpy())
y_true = np.concatenate(y_true, axis=0)
y_true = np.argmax(y_true, axis=1)

y_pred_prob = model.predict(test_ds)
y_pred = np.argmax(y_pred_prob, axis=1)

# Report
print("\nClassification Report:\n")
print(classification_report(y_true, y_pred, target_names=class_names, digits=4))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,6))
plt.imshow(cm, cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.colorbar()
ticks = np.arange(len(class_names))
plt.xticks(ticks, class_names, rotation=45)
plt.yticks(ticks, class_names)

thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, cm[i, j], horizontalalignment="center",
             color="white" if cm[i, j] > thresh else "black")

plt.ylabel("True label")
plt.xlabel("Predicted label")
plt.tight_layout()
plt.show()

